import pyspark.pandas as ks
import dxpy
import dxdata
import pandas as pd
import pyspark
import re

# Initialize Spark (run once)
sc = pyspark.SparkContext.getOrCreate()
spark = pyspark.sql.SparkSession(sc)

dispensed_dataset = dxpy.find_one_data_object(
    typename="Dataset",
    name="app*.dataset",
    folder="/",
    name_mode="glob"
)

dispensed_dataset_id = dispensed_dataset["id"]
dataset = dxdata.load_dataset(id=dispensed_dataset_id)

participant = dataset["participant"]

participant

field_ids = ['31', '22001', '22006', '22019', '22021', '21022']


case = dxdata.load_cohort("YT_HA", folder="/Data")
cont = dxdata.load_cohort("Not in YT_HA", folder="/Data")

def fields_for_id(field_id):
    from distutils.version import LooseVersion
    field_id = str(field_id)
    fields = participant.find_fields(
        name_regex=r'^p{}(_i\d+)?(_a\d+)?$'.format(field_id)
    )
    return sorted(fields, key=lambda f: LooseVersion(f.name))
fields = [fields_for_id(f)[0] for f in field_ids] + [
    participant.find_field(name='eid')
]

case_df = participant.retrieve_fields(
    fields=fields,
    filter_sql=case.sql,
    engine=dxdata.connect()
).to_koalas()

cont_df = participant.retrieve_fields(
    fields=fields,
    filter_sql=cont.sql,
    engine=dxdata.connect(
        dialect="hive+pyspark",
        connect_args={
            "config": {
                "spark.kryoserializer.buffer.max": "256m",
                "spark.sql.autoBroadcastJoinThreshold": "-1"
            }
        }
    )
).to_koalas()

case_df['ha_cc'] = 1
cont_df['ha_cc'] = 0


df = ks.concat([case_df, cont_df])
df.shape

df.ha_cc.value_counts()

df_qced = df[
    (df['p31'] == df['p22001']) &
    (df['p22019'].isnull())
]

df_qced = df_qced.rename(columns={
    'eid': 'IID',
    'p31': 'sex',
    'p21022': 'age'
})

df_qced['FID'] = df_qced['IID']

df_qced.ha_cc.value_counts()
#0 458828
#1 180

df_phenotype = df_qced[['FID', 'IID', 'ha_cc', 'sex', 'age']].to_pandas()
df_phenotype.head()

out_path_local = "/home/dnanexus/HA_YT.phen.tsv"
df_phenotype.to_csv(out_path_local, sep="\t", index=False, na_rep="NA")
print("Saved locally:", out_path_local)

!dx upload "/home/dnanexus/HA_YT.phen.tsv" --destination "/Data/HA_YT.phen.tsv" --brief
!dx find data --name "HA_YT.phen.tsv" --path "/" --brief
!dx ls "/Data"


# Downsample to 1:10
import pandas as pd

df = pd.read_csv("/home/dnanexus/HA_YT.phen.tsv", sep="\t")

cases = df[df.ha_cc == 1]
controls = df[df.ha_cc == 0].sample(n=10 * len(cases), random_state=42)

df_ds = pd.concat([cases, controls])
df_ds.ha_cc.value_counts()

df_ds.to_csv("/home/dnanexus/HA_YT_1to10.phen.tsv", sep="\t", index=False)

!dx upload "/home/dnanexus/HA_YT_1to10.phen.tsv" --destination "/Data/HA_YT_1to10.phen.tsv" --brief
!dx find data --name "HA_YT_1to10.phen.tsv" --path "/" --brief
!dx ls "/Data"

# Downsample to 1:20
import pandas as pd
df = pd.read_csv("/home/dnanexus/HA_YT.phen.tsv", sep="\t")

# Split cases / controls
cases = df[df.ha_cc == 1]
controls_all = df[df.ha_cc == 0]

n_cases = len(cases)
n_ctrl_target = 20 * n_cases

print("Cases:", n_cases)
print("Available controls:", len(controls_all))
print("Target controls (1:20):", n_ctrl_target)

# Safety check
if len(controls_all) < n_ctrl_target:
    raise ValueError("Not enough controls to sample 1:20")

# Sample controls
controls = controls_all.sample(
    n=n_ctrl_target,
    random_state=42
)

# Combine
df_ds = pd.concat([cases, controls], axis=0)

# Sanity check
print(df_ds.ha_cc.value_counts())

# Write out
out_path = "/home/dnanexus/HA_YT_1to20.phen.tsv"
df_ds.to_csv(out_path, sep="\t", index=False)

print("Wrote:", out_path)


